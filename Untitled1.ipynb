{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5fd41c-0278-4f97-83de-b7a1c6ffea70",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deque, Counter\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ks_2samp\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# -----------------------------------\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Setup logging\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# -----------------------------------\u001b[39;00m\n\u001b[32m     11\u001b[39m logging.basicConfig(\n\u001b[32m     12\u001b[39m     filename=\u001b[33m\"\u001b[39m\u001b[33mmonitoring.log\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     level=logging.INFO,\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mformat\u001b[39m=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import logging\n",
    "from collections import deque, Counter\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# -----------------------------------\n",
    "# Setup logging\n",
    "# -----------------------------------\n",
    "logging.basicConfig(\n",
    "    filename=\"monitoring.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# -----------------------------------\n",
    "# Load training metrics\n",
    "# -----------------------------------\n",
    "with open(\"../training_metrics.json\", \"r\") as f:\n",
    "    training_metrics = json.load(f)\n",
    "\n",
    "train_action_dist = np.array(training_metrics[\"train_action_dist\"])\n",
    "train_wait_red = np.array(training_metrics[\"train_wait_red\"])\n",
    "train_wait_yellow = np.array(training_metrics[\"train_wait_yellow\"])\n",
    "\n",
    "# -----------------------------------\n",
    "# Buffers for live monitoring\n",
    "# -----------------------------------\n",
    "recent_rewards = deque(maxlen=200)\n",
    "recent_actions = deque(maxlen=200)\n",
    "recent_waits_red = deque(maxlen=200)\n",
    "recent_waits_yellow = deque(maxlen=200)\n",
    "\n",
    "# -----------------------------------\n",
    "# Reward tracking\n",
    "# -----------------------------------\n",
    "def track_reward(reward):\n",
    "    recent_rewards.append(reward)\n",
    "    logging.info(f\"Reward logged: {reward}\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Action tracking (Model Drift)\n",
    "# -----------------------------------\n",
    "def track_action(action):\n",
    "    \"\"\"\n",
    "    action: 0=Red (urgent), 1=Yellow (non-urgent)\n",
    "    \"\"\"\n",
    "    recent_actions.append(action)\n",
    "    count = Counter(recent_actions)\n",
    "    total = sum(count.values())\n",
    "    dist = {\n",
    "        \"red\": count.get(0, 0)/total if total else 0,\n",
    "        \"yellow\": count.get(1, 0)/total if total else 0\n",
    "    }\n",
    "    logging.info(f\"Current action distribution: {dist}\")\n",
    "\n",
    "    # Model drift detection (Euclidean distance)\n",
    "    action_vector = np.array([dist[\"red\"], dist[\"yellow\"]])\n",
    "    drift = np.linalg.norm(action_vector - train_action_dist)\n",
    "    if drift > 0.25:\n",
    "        logging.warning(\"MODEL DRIFT DETECTED: Action distribution deviates from training behavior!\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Wait-time tracking (Data Drift)\n",
    "# -----------------------------------\n",
    "def track_wait_time(cat, wait_time):\n",
    "    \"\"\"\n",
    "    cat: \"red\" or \"yellow\"\n",
    "    wait_time: float/int\n",
    "    \"\"\"\n",
    "    if cat == \"red\":\n",
    "        recent_waits_red.append(wait_time)\n",
    "        if len(recent_waits_red) > 30:\n",
    "            _check_wait_drift(\"red\")\n",
    "    else:\n",
    "        recent_waits_yellow.append(wait_time)\n",
    "        if len(recent_waits_yellow) > 30:\n",
    "            _check_wait_drift(\"yellow\")\n",
    "\n",
    "def _check_wait_drift(cat):\n",
    "    if cat == \"red\":\n",
    "        stat, p = ks_2samp(train_wait_red, list(recent_waits_red))\n",
    "    else:\n",
    "        stat, p = ks_2samp(train_wait_yellow, list(recent_waits_yellow))\n",
    "    \n",
    "    if p < 0.05:\n",
    "        logging.warning(f\"DATA DRIFT DETECTED in {cat.upper()} wait times (p={p:.4f})\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Log every decision\n",
    "# -----------------------------------\n",
    "def log_decision(obs, action, reward, info={}):\n",
    "    \"\"\"\n",
    "    obs: environment observation array\n",
    "    action: 0=Red, 1=Yellow\n",
    "    reward: float\n",
    "    info: optional dict\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"timestamp\": time.time(),\n",
    "        \"action\": int(action),\n",
    "        \"reward\": float(reward),\n",
    "        \"free_doctors\": int(obs[0]),\n",
    "        \"longest_wait_red\": float(obs[1]),\n",
    "        \"longest_wait_yellow\": float(obs[2]),\n",
    "        \"red_queue_len\": int(obs[3]),\n",
    "        \"yellow_queue_len\": int(obs[4]),\n",
    "        \"doctor_busy_times\": [float(obs[5]), float(obs[6]), float(obs[7])],\n",
    "        \"additional_info\": info\n",
    "    }\n",
    "    logging.info(\"Decision: \" + json.dumps(data))\n",
    "\n",
    "# -----------------------------------\n",
    "# Example usage (replace with your live environment loop)\n",
    "# -----------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate some sample decisions\n",
    "    for i in range(50):\n",
    "        # Fake observation\n",
    "        obs = np.array([2, 3, 5, 1, 6, 0, 0, 1], dtype=float)\n",
    "        action = np.random.choice([0, 1])\n",
    "        reward = np.random.rand() * 50\n",
    "        track_reward(reward)\n",
    "        track_action(action)\n",
    "        track_wait_time(\"red\" if action==0 else \"yellow\", np.random.rand()*20)\n",
    "        log_decision(obs, action, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c61cb4a-e3fb-42da-82f3-1e696818233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import logging\n",
    "from collections import deque, Counter\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "import os\n",
    "\n",
    "# -----------------------------------\n",
    "# Setup logging\n",
    "# -----------------------------------\n",
    "logging.basicConfig(\n",
    "    filename=\"monitoring.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# -----------------------------------\n",
    "# Load training metrics\n",
    "# -----------------------------------\n",
    "root_dir = \"C:/Users/Prudence Letaru/Desktop/RL_Project_New\"\n",
    "with open(os.path.join(root_dir, \"training_metrics.json\"), \"r\") as f:\n",
    "    training_metrics = json.load(f)\n",
    "\n",
    "train_action_dist = np.array(training_metrics[\"train_action_dist\"])\n",
    "train_wait_red = np.array(training_metrics[\"train_wait_red\"])\n",
    "train_wait_yellow = np.array(training_metrics[\"train_wait_yellow\"])\n",
    "\n",
    "# -----------------------------------\n",
    "# Buffers for live monitoring\n",
    "# -----------------------------------\n",
    "recent_rewards = deque(maxlen=200)\n",
    "recent_actions = deque(maxlen=200)\n",
    "recent_waits_red = deque(maxlen=200)\n",
    "recent_waits_yellow = deque(maxlen=200)\n",
    "\n",
    "# -----------------------------------\n",
    "# Reward tracking\n",
    "# -----------------------------------\n",
    "def track_reward(reward):\n",
    "    recent_rewards.append(reward)\n",
    "    logging.info(f\"Reward logged: {reward}\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Action tracking (Model Drift)\n",
    "# -----------------------------------\n",
    "def track_action(action):\n",
    "    \"\"\"\n",
    "    action: 0=Red (urgent), 1=Yellow (non-urgent)\n",
    "    \"\"\"\n",
    "    recent_actions.append(action)\n",
    "    count = Counter(recent_actions)\n",
    "    total = sum(count.values())\n",
    "    dist = {\n",
    "        \"red\": count.get(0, 0)/total if total else 0,\n",
    "        \"yellow\": count.get(1, 0)/total if total else 0\n",
    "    }\n",
    "    logging.info(f\"Current action distribution: {dist}\")\n",
    "\n",
    "    # Model drift detection (Euclidean distance)\n",
    "    action_vector = np.array([dist[\"red\"], dist[\"yellow\"]])\n",
    "    drift = np.linalg.norm(action_vector - train_action_dist)\n",
    "    if drift > 0.25:\n",
    "        logging.warning(\"MODEL DRIFT DETECTED: Action distribution deviates from training behavior!\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Wait-time tracking (Data Drift)\n",
    "# -----------------------------------\n",
    "def track_wait_time(cat, wait_time):\n",
    "    \"\"\"\n",
    "    cat: \"red\" or \"yellow\"\n",
    "    wait_time: float/int\n",
    "    \"\"\"\n",
    "    if cat == \"red\":\n",
    "        recent_waits_red.append(wait_time)\n",
    "        if len(recent_waits_red) > 30:\n",
    "            _check_wait_drift(\"red\")\n",
    "    else:\n",
    "        recent_waits_yellow.append(wait_time)\n",
    "        if len(recent_waits_yellow) > 30:\n",
    "            _check_wait_drift(\"yellow\")\n",
    "\n",
    "def _check_wait_drift(cat):\n",
    "    if cat == \"red\":\n",
    "        stat, p = ks_2samp(train_wait_red, list(recent_waits_red))\n",
    "    else:\n",
    "        stat, p = ks_2samp(train_wait_yellow, list(recent_waits_yellow))\n",
    "    \n",
    "    if p < 0.05:\n",
    "        logging.warning(f\"DATA DRIFT DETECTED in {cat.upper()} wait times (p={p:.4f})\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Log every decision\n",
    "# -----------------------------------\n",
    "def log_decision(obs, action, reward, info={}):\n",
    "    \"\"\"\n",
    "    obs: environment observation array\n",
    "    action: 0=Red, 1=Yellow\n",
    "    reward: float\n",
    "    info: optional dict\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"timestamp\": time.time(),\n",
    "        \"action\": int(action),\n",
    "        \"reward\": float(reward),\n",
    "        \"free_doctors\": int(obs[0]),\n",
    "        \"longest_wait_red\": float(obs[1]),\n",
    "        \"longest_wait_yellow\": float(obs[2]),\n",
    "        \"red_queue_len\": int(obs[3]),\n",
    "        \"yellow_queue_len\": int(obs[4]),\n",
    "        \"doctor_busy_times\": [float(obs[5]), float(obs[6]), float(obs[7])],\n",
    "        \"additional_info\": info\n",
    "    }\n",
    "    logging.info(\"Decision: \" + json.dumps(data))\n",
    "\n",
    "# -----------------------------------\n",
    "# Example usage (replace with your live environment loop)\n",
    "# -----------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate some sample decisions\n",
    "    for i in range(50):\n",
    "        # Fake observation\n",
    "        obs = np.array([2, 3, 5, 1, 6, 0, 0, 1], dtype=float)\n",
    "        action = np.random.choice([0, 1])\n",
    "        reward = np.random.rand() * 50\n",
    "        track_reward(reward)\n",
    "        track_action(action)\n",
    "        track_wait_time(\"red\" if action==0 else \"yellow\", np.random.rand()*20)\n",
    "        log_decision(obs, action, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64649c90-515c-415a-a202-6e5a2a96d3b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type ndarray is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 168\u001b[39m\n\u001b[32m    165\u001b[39m     track_wait_time(\u001b[33m\"\u001b[39m\u001b[33myellow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(env.last_served_wait_times[\u001b[33m\"\u001b[39m\u001b[33myellow\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# Log decision\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m \u001b[43mlog_decision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# Queue lengths\u001b[39;00m\n\u001b[32m    171\u001b[39m queue_lengths[\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m].append(\u001b[38;5;28mlen\u001b[39m(env.red_queue))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mlog_decision\u001b[39m\u001b[34m(obs, action, reward, info)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_decision\u001b[39m(obs, action, reward, info={}):\n\u001b[32m     75\u001b[39m     data = {\n\u001b[32m     76\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m: time.time(),\n\u001b[32m     77\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maction\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(action),\n\u001b[32m   (...)\u001b[39m\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33madditional_info\u001b[39m\u001b[33m\"\u001b[39m: info\n\u001b[32m     86\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     logging.info(\u001b[33m\"\u001b[39m\u001b[33mDecision: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\rl_env\\Lib\\json\\__init__.py:231\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# cached encoder\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    228\u001b[39m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    230\u001b[39m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONEncoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\rl_env\\Lib\\json\\encoder.py:200\u001b[39m, in \u001b[36mJSONEncoder.encode\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    202\u001b[39m     chunks = \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\rl_env\\Lib\\json\\encoder.py:258\u001b[39m, in \u001b[36mJSONEncoder.iterencode\u001b[39m\u001b[34m(self, o, _one_shot)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    254\u001b[39m     _iterencode = _make_iterencode(\n\u001b[32m    255\u001b[39m         markers, \u001b[38;5;28mself\u001b[39m.default, _encoder, \u001b[38;5;28mself\u001b[39m.indent, floatstr,\n\u001b[32m    256\u001b[39m         \u001b[38;5;28mself\u001b[39m.key_separator, \u001b[38;5;28mself\u001b[39m.item_separator, \u001b[38;5;28mself\u001b[39m.sort_keys,\n\u001b[32m    257\u001b[39m         \u001b[38;5;28mself\u001b[39m.skipkeys, _one_shot)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\rl_env\\Lib\\json\\encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type ndarray is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from collections import deque, Counter\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# -------------------------------\n",
    "# Setup logging\n",
    "# -------------------------------\n",
    "logging.basicConfig(\n",
    "    filename=\"monitoring.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Global buffers for drift detection\n",
    "# -------------------------------\n",
    "recent_rewards = deque(maxlen=200)\n",
    "recent_actions = deque(maxlen=200)\n",
    "recent_wait_red = deque(maxlen=200)\n",
    "recent_wait_yellow = deque(maxlen=200)\n",
    "\n",
    "# -------------------------------\n",
    "# Monitoring functions\n",
    "# -------------------------------\n",
    "def track_reward(reward):\n",
    "    recent_rewards.append(reward)\n",
    "    logging.info(f\"Reward logged: {reward}\")\n",
    "\n",
    "def track_action(action):\n",
    "    recent_actions.append(action)\n",
    "    count = Counter(recent_actions)\n",
    "    total = sum(count.values())\n",
    "    dist = {\n",
    "        \"red\": count.get(0, 0) / total if total else 0,\n",
    "        \"yellow\": count.get(1, 0) / total if total else 0\n",
    "    }\n",
    "    logging.info(f\"Action distribution: {dist}\")\n",
    "\n",
    "    # Drift detection\n",
    "    action_vector = np.array([dist[\"red\"], dist[\"yellow\"]])\n",
    "    if \"train_action_dist\" in globals():\n",
    "        drift = np.linalg.norm(action_vector - train_action_dist)\n",
    "        if drift > 0.25:\n",
    "            logging.warning(\"MODEL DRIFT DETECTED: Action distribution deviates from training!\")\n",
    "\n",
    "def track_wait_time(cat, wait_time):\n",
    "    if cat == \"red\":\n",
    "        recent_wait_red.append(wait_time)\n",
    "        if len(recent_wait_red) > 30:\n",
    "            _check_wait_time_drift(\"red\")\n",
    "    else:\n",
    "        recent_wait_yellow.append(wait_time)\n",
    "        if len(recent_wait_yellow) > 30:\n",
    "            _check_wait_time_drift(\"yellow\")\n",
    "\n",
    "def _check_wait_time_drift(cat):\n",
    "    if cat == \"red\":\n",
    "        stat, p = ks_2samp(train_wait_red, list(recent_wait_red))\n",
    "    else:\n",
    "        stat, p = ks_2samp(train_wait_yellow, list(recent_wait_yellow))\n",
    "\n",
    "    if p < 0.05:\n",
    "        logging.warning(f\"DATA DRIFT DETECTED in {cat.upper()} wait times (p={p:.4f})\")\n",
    "\n",
    "def log_decision(obs, action, reward, info={}):\n",
    "    data = {\n",
    "        \"timestamp\": time.time(),\n",
    "        \"action\": int(action),\n",
    "        \"reward\": float(reward),\n",
    "        \"free_doctors\": int(obs[0]),\n",
    "        \"longest_wait_red\": float(obs[1]),\n",
    "        \"longest_wait_yellow\": float(obs[2]),\n",
    "        \"red_queue_len\": int(obs[3]),\n",
    "        \"yellow_queue_len\": int(obs[4]),\n",
    "        \"doctor_busy_times\": [float(obs[5]), float(obs[6]), float(obs[7])],\n",
    "        \"additional_info\": info\n",
    "    }\n",
    "    logging.info(\"Decision: \" + json.dumps(data))\n",
    "\n",
    "# -------------------------------\n",
    "# Main loop: run actual environment\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Add environment path\n",
    "    root_dir = \"C:/Users/Prudence Letaru/Desktop/RL_Project_New\"\n",
    "    sys.path.append(root_dir)\n",
    "\n",
    "    sys.path.append(os.path.join(root_dir, \"env\"))\n",
    "    from hospital_env import HospitalEnv\n",
    "\n",
    "    # Create evaluation environment\n",
    "    def make_env():\n",
    "        env = HospitalEnv()\n",
    "        env = Monitor(env)\n",
    "        return env\n",
    "\n",
    "    eval_env = DummyVecEnv([make_env])\n",
    "\n",
    "    # Load trained model\n",
    "    model = DQN.load(\"models/dqn_hospital_sb3\", env=eval_env)\n",
    "\n",
    "    # Load previous training metrics if available\n",
    "    if os.path.exists(\"training_metrics.json\"):\n",
    "        with open(\"training_metrics.json\", \"r\") as f:\n",
    "            training_metrics = json.load(f)\n",
    "        train_action_dist = np.array(training_metrics[\"train_action_dist\"])\n",
    "        train_wait_red = np.array(training_metrics[\"train_wait_red\"])\n",
    "        train_wait_yellow = np.array(training_metrics[\"train_wait_yellow\"])\n",
    "    else:\n",
    "        # fallback: uniform distribution\n",
    "        train_action_dist = np.array([0.5, 0.5])\n",
    "        train_wait_red = np.random.normal(10, 3, 500)\n",
    "        train_wait_yellow = np.random.normal(20, 5, 500)\n",
    "\n",
    "    # Evaluation parameters\n",
    "    n_episodes = 40\n",
    "    threshold_times = {\"red\": 15, \"yellow\": 30}\n",
    "\n",
    "    # Metrics storage\n",
    "    rewards_per_episode = []\n",
    "    red_waits, yellow_waits = [], []\n",
    "    queue_lengths = {\"red\": [], \"yellow\": []}\n",
    "    total_actions_red, total_actions_yellow = 0, 0\n",
    "\n",
    "    # -------------------------------\n",
    "    # Run episodes\n",
    "    # -------------------------------\n",
    "    for ep in range(n_episodes):\n",
    "        obs = eval_env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "\n",
    "            if action[0] == 0:\n",
    "                total_actions_red += 1\n",
    "            else:\n",
    "                total_actions_yellow += 1\n",
    "\n",
    "            obs, reward, done, info = eval_env.step(action)\n",
    "            episode_reward += float(reward[0])\n",
    "\n",
    "            env = eval_env.envs[0].unwrapped\n",
    "\n",
    "            # Track metrics\n",
    "            track_reward(float(reward[0]))\n",
    "            track_action(action[0])\n",
    "\n",
    "            # Track wait times only if someone was served\n",
    "            if env.last_served_wait_times[\"red\"] > 0:\n",
    "                red_waits.append(float(env.last_served_wait_times[\"red\"]))\n",
    "                track_wait_time(\"red\", float(env.last_served_wait_times[\"red\"]))\n",
    "            if env.last_served_wait_times[\"yellow\"] > 0:\n",
    "                yellow_waits.append(float(env.last_served_wait_times[\"yellow\"]))\n",
    "                track_wait_time(\"yellow\", float(env.last_served_wait_times[\"yellow\"]))\n",
    "\n",
    "            # Log decision\n",
    "            log_decision(obs[0], action[0], float(reward[0]), info)\n",
    "\n",
    "            # Queue lengths\n",
    "            queue_lengths[\"red\"].append(len(env.red_queue))\n",
    "            queue_lengths[\"yellow\"].append(len(env.yellow_queue))\n",
    "\n",
    "        rewards_per_episode.append(episode_reward)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Compute summary metrics\n",
    "    # -------------------------------\n",
    "    avg_reward = float(np.mean(rewards_per_episode))\n",
    "    avg_wait_red = float(np.mean(red_waits)) if red_waits else 0\n",
    "    avg_wait_yellow = float(np.mean(yellow_waits)) if yellow_waits else 0\n",
    "    pct_red_within = float(100 * sum(w <= threshold_times[\"red\"] for w in red_waits) / len(red_waits)) if red_waits else 0\n",
    "    pct_yellow_within = float(100 * sum(w <= threshold_times[\"yellow\"] for w in yellow_waits) / len(yellow_waits)) if yellow_waits else 0\n",
    "    queue_stats = {cat: {\"avg\": float(np.mean(qs)), \"max\": int(np.max(qs))} for cat, qs in queue_lengths.items()}\n",
    "\n",
    "    # Action distribution\n",
    "    total_actions = total_actions_red + total_actions_yellow\n",
    "    red_pct = float(total_actions_red / total_actions) if total_actions > 0 else 0\n",
    "    yellow_pct = float(total_actions_yellow / total_actions) if total_actions > 0 else 0\n",
    "\n",
    "    print(\"\\n=== ACTION DISTRIBUTION ===\")\n",
    "    print(f\"Red actions: {total_actions_red} ({red_pct:.3f})\")\n",
    "    print(f\"Yellow actions: {total_actions_yellow} ({yellow_pct:.3f})\")\n",
    "    print(\"\\n=== PERFORMANCE METRICS ===\")\n",
    "    print(f\"Average reward per episode: {avg_reward:.2f}\")\n",
    "    print(f\"Average wait times (Red, Yellow): {avg_wait_red:.2f}, {avg_wait_yellow:.2f}\")\n",
    "    print(f\"Percentage served within thresholds (Red, Yellow): {pct_red_within:.2f}%, {pct_yellow_within:.2f}%\")\n",
    "    for cat, stats in queue_stats.items():\n",
    "        print(f\"{cat.capitalize()}: avg={stats['avg']:.2f}, max={stats['max']}\")\n",
    "\n",
    "    # Save metrics\n",
    "    training_metrics = {\n",
    "        \"train_action_dist\": [red_pct, yellow_pct],\n",
    "        \"train_wait_red\": red_waits,\n",
    "        \"train_wait_yellow\": yellow_waits,\n",
    "        \"avg_reward_per_episode\": avg_reward,\n",
    "        \"pct_within_threshold_red\": pct_red_within,\n",
    "        \"pct_within_threshold_yellow\": pct_yellow_within,\n",
    "        \"queue_red_avg\": queue_stats[\"red\"][\"avg\"],\n",
    "        \"queue_red_max\": queue_stats[\"red\"][\"max\"],\n",
    "        \"queue_yellow_avg\": queue_stats[\"yellow\"][\"avg\"],\n",
    "        \"queue_yellow_max\": queue_stats[\"yellow\"][\"max\"]\n",
    "    }\n",
    "\n",
    "    with open(\"training_metrics.json\", \"w\") as f:\n",
    "        json.dump(training_metrics, f, indent=4)\n",
    "\n",
    "    print(\"\\n[INFO] Monitoring complete. Logs in 'monitoring.log'. Metrics saved in 'training_metrics.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8585929-f33a-4b3c-9fc3-9defb25f6372",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type ndarray is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 171\u001b[39m\n\u001b[32m    168\u001b[39m     track_wait_time(\u001b[33m\"\u001b[39m\u001b[33myellow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(env.last_served_wait_times[\u001b[33m\"\u001b[39m\u001b[33myellow\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# Log decision\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[43mlog_decision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# Queue lengths\u001b[39;00m\n\u001b[32m    174\u001b[39m queue_lengths[\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m].append(\u001b[38;5;28mlen\u001b[39m(env.red_queue))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mlog_decision\u001b[39m\u001b[34m(obs, action, reward, info)\u001b[39m\n\u001b[32m     76\u001b[39m obs_list = obs.tolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, np.ndarray) \u001b[38;5;28;01melse\u001b[39;00m obs\n\u001b[32m     77\u001b[39m data = {\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m: time.time(),\n\u001b[32m     79\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maction\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(action),\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33madditional_info\u001b[39m\u001b[33m\"\u001b[39m: info\n\u001b[32m     88\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mDecision: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\rl_env\\Lib\\json\\__init__.py:231\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# cached encoder\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    228\u001b[39m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    230\u001b[39m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONEncoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\rl_env\\Lib\\json\\encoder.py:200\u001b[39m, in \u001b[36mJSONEncoder.encode\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    202\u001b[39m     chunks = \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\rl_env\\Lib\\json\\encoder.py:258\u001b[39m, in \u001b[36mJSONEncoder.iterencode\u001b[39m\u001b[34m(self, o, _one_shot)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    254\u001b[39m     _iterencode = _make_iterencode(\n\u001b[32m    255\u001b[39m         markers, \u001b[38;5;28mself\u001b[39m.default, _encoder, \u001b[38;5;28mself\u001b[39m.indent, floatstr,\n\u001b[32m    256\u001b[39m         \u001b[38;5;28mself\u001b[39m.key_separator, \u001b[38;5;28mself\u001b[39m.item_separator, \u001b[38;5;28mself\u001b[39m.sort_keys,\n\u001b[32m    257\u001b[39m         \u001b[38;5;28mself\u001b[39m.skipkeys, _one_shot)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\rl_env\\Lib\\json\\encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type ndarray is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from collections import deque, Counter\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# -------------------------------\n",
    "# Setup logging\n",
    "# -------------------------------\n",
    "logging.basicConfig(\n",
    "    filename=\"monitoring.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Global buffers for drift detection\n",
    "# -------------------------------\n",
    "recent_rewards = deque(maxlen=200)\n",
    "recent_actions = deque(maxlen=200)\n",
    "recent_wait_red = deque(maxlen=200)\n",
    "recent_wait_yellow = deque(maxlen=200)\n",
    "\n",
    "# -------------------------------\n",
    "# Monitoring functions\n",
    "# -------------------------------\n",
    "def track_reward(reward):\n",
    "    recent_rewards.append(reward)\n",
    "    logging.info(f\"Reward logged: {reward}\")\n",
    "\n",
    "def track_action(action):\n",
    "    recent_actions.append(action)\n",
    "    count = Counter(recent_actions)\n",
    "    total = sum(count.values())\n",
    "    dist = {\n",
    "        \"red\": count.get(0, 0) / total if total else 0,\n",
    "        \"yellow\": count.get(1, 0) / total if total else 0\n",
    "    }\n",
    "    logging.info(f\"Action distribution: {dist}\")\n",
    "\n",
    "    # Drift detection\n",
    "    action_vector = np.array([dist[\"red\"], dist[\"yellow\"]])\n",
    "    if \"train_action_dist\" in globals():\n",
    "        drift = np.linalg.norm(action_vector - train_action_dist)\n",
    "        if drift > 0.25:\n",
    "            logging.warning(\"MODEL DRIFT DETECTED: Action distribution deviates from training!\")\n",
    "\n",
    "def track_wait_time(cat, wait_time):\n",
    "    if cat == \"red\":\n",
    "        recent_wait_red.append(wait_time)\n",
    "        if len(recent_wait_red) > 30:\n",
    "            _check_wait_time_drift(\"red\")\n",
    "    else:\n",
    "        recent_wait_yellow.append(wait_time)\n",
    "        if len(recent_wait_yellow) > 30:\n",
    "            _check_wait_time_drift(\"yellow\")\n",
    "\n",
    "def _check_wait_time_drift(cat):\n",
    "    if cat == \"red\":\n",
    "        stat, p = ks_2samp(train_wait_red, list(recent_wait_red))\n",
    "    else:\n",
    "        stat, p = ks_2samp(train_wait_yellow, list(recent_wait_yellow))\n",
    "\n",
    "    if p < 0.05:\n",
    "        logging.warning(f\"DATA DRIFT DETECTED in {cat.upper()} wait times (p={p:.4f})\")\n",
    "\n",
    "def log_decision(obs, action, reward, info={}):\n",
    "    # Convert obs to list if it's a numpy array\n",
    "    obs_list = obs.tolist() if isinstance(obs, np.ndarray) else obs\n",
    "    data = {\n",
    "        \"timestamp\": time.time(),\n",
    "        \"action\": int(action),\n",
    "        \"reward\": float(reward),\n",
    "        \"free_doctors\": int(obs_list[0]),\n",
    "        \"longest_wait_red\": float(obs_list[1]),\n",
    "        \"longest_wait_yellow\": float(obs_list[2]),\n",
    "        \"red_queue_len\": int(obs_list[3]),\n",
    "        \"yellow_queue_len\": int(obs_list[4]),\n",
    "        \"doctor_busy_times\": [float(obs_list[5]), float(obs_list[6]), float(obs_list[7])],\n",
    "        \"additional_info\": info\n",
    "    }\n",
    "    logging.info(\"Decision: \" + json.dumps(data))\n",
    "\n",
    "# -------------------------------\n",
    "# Main loop: run actual environment\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths\n",
    "    root_dir = \"C:/Users/Prudence Letaru/Desktop/RL_Project_New\"\n",
    "    sys.path.append(root_dir)\n",
    "    sys.path.append(os.path.join(root_dir, \"env\"))\n",
    "\n",
    "    # Import environment\n",
    "    from hospital_env import HospitalEnv\n",
    "\n",
    "    # Create evaluation environment\n",
    "    def make_env():\n",
    "        env = HospitalEnv()\n",
    "        env = Monitor(env)\n",
    "        return env\n",
    "\n",
    "    eval_env = DummyVecEnv([make_env])\n",
    "\n",
    "    # Load trained model\n",
    "    model = DQN.load(os.path.join(root_dir, \"models/dqn_hospital_sb3\"), env=eval_env)\n",
    "\n",
    "    # Load previous training metrics if available\n",
    "    metrics_path = os.path.join(root_dir, \"training_metrics.json\")\n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path, \"r\") as f:\n",
    "            training_metrics = json.load(f)\n",
    "        train_action_dist = np.array(training_metrics[\"train_action_dist\"])\n",
    "        train_wait_red = np.array(training_metrics[\"train_wait_red\"])\n",
    "        train_wait_yellow = np.array(training_metrics[\"train_wait_yellow\"])\n",
    "    else:\n",
    "        train_action_dist = np.array([0.5, 0.5])\n",
    "        train_wait_red = np.random.normal(10, 3, 500)\n",
    "        train_wait_yellow = np.random.normal(20, 5, 500)\n",
    "\n",
    "    # Evaluation parameters\n",
    "    n_episodes = 40\n",
    "    threshold_times = {\"red\": 15, \"yellow\": 30}\n",
    "\n",
    "    # Metrics storage\n",
    "    rewards_per_episode = []\n",
    "    red_waits, yellow_waits = [], []\n",
    "    queue_lengths = {\"red\": [], \"yellow\": []}\n",
    "    total_actions_red, total_actions_yellow = 0, 0\n",
    "\n",
    "    # -------------------------------\n",
    "    # Run episodes\n",
    "    # -------------------------------\n",
    "    for ep in range(n_episodes):\n",
    "        obs = eval_env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "\n",
    "            if action[0] == 0:\n",
    "                total_actions_red += 1\n",
    "            else:\n",
    "                total_actions_yellow += 1\n",
    "\n",
    "            obs, reward, done, info = eval_env.step(action)\n",
    "            episode_reward += float(reward[0])\n",
    "\n",
    "            env = eval_env.envs[0].unwrapped\n",
    "\n",
    "            # Track metrics\n",
    "            track_reward(float(reward[0]))\n",
    "            track_action(action[0])\n",
    "\n",
    "            # Track wait times only if someone was served\n",
    "            if env.last_served_wait_times[\"red\"] > 0:\n",
    "                red_waits.append(float(env.last_served_wait_times[\"red\"]))\n",
    "                track_wait_time(\"red\", float(env.last_served_wait_times[\"red\"]))\n",
    "            if env.last_served_wait_times[\"yellow\"] > 0:\n",
    "                yellow_waits.append(float(env.last_served_wait_times[\"yellow\"]))\n",
    "                track_wait_time(\"yellow\", float(env.last_served_wait_times[\"yellow\"]))\n",
    "\n",
    "            # Log decision\n",
    "            log_decision(obs[0], action[0], float(reward[0]), info)\n",
    "\n",
    "            # Queue lengths\n",
    "            queue_lengths[\"red\"].append(len(env.red_queue))\n",
    "            queue_lengths[\"yellow\"].append(len(env.yellow_queue))\n",
    "\n",
    "        rewards_per_episode.append(episode_reward)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Compute summary metrics\n",
    "    # -------------------------------\n",
    "    avg_reward = float(np.mean(rewards_per_episode))\n",
    "    avg_wait_red = float(np.mean(red_waits)) if red_waits else 0\n",
    "    avg_wait_yellow = float(np.mean(yellow_waits)) if yellow_waits else 0\n",
    "    pct_red_within = float(100 * sum(w <= threshold_times[\"red\"] for w in red_waits) / len(red_waits)) if red_waits else 0\n",
    "    pct_yellow_within = float(100 * sum(w <= threshold_times[\"yellow\"] for w in yellow_waits) / len(yellow_waits)) if yellow_waits else 0\n",
    "    queue_stats = {cat: {\"avg\": float(np.mean(qs)), \"max\": int(np.max(qs))} for cat, qs in queue_lengths.items()}\n",
    "\n",
    "    # Action distribution\n",
    "    total_actions = total_actions_red + total_actions_yellow\n",
    "    red_pct = float(total_actions_red / total_actions) if total_actions > 0 else 0\n",
    "    yellow_pct = float(total_actions_yellow / total_actions) if total_actions > 0 else 0\n",
    "\n",
    "    # -------------------------------\n",
    "    # Print summary\n",
    "    # -------------------------------\n",
    "    print(\"\\n=== ACTION DISTRIBUTION ===\")\n",
    "    print(f\"Red actions: {total_actions_red} ({red_pct:.3f})\")\n",
    "    print(f\"Yellow actions: {total_actions_yellow} ({yellow_pct:.3f})\")\n",
    "    print(\"\\n=== PERFORMANCE METRICS ===\")\n",
    "    print(f\"Average reward per episode: {avg_reward:.2f}\")\n",
    "    print(f\"Average wait times (Red, Yellow): {avg_wait_red:.2f}, {avg_wait_yellow:.2f}\")\n",
    "    print(f\"Percentage served within thresholds (Red, Yellow): {pct_red_within:.2f}%, {pct_yellow_within:.2f}%\")\n",
    "    for cat, stats in queue_stats.items():\n",
    "        print(f\"{cat.capitalize()}: avg={stats['avg']:.2f}, max={stats['max']}\")\n",
    "\n",
    "    # Save metrics\n",
    "    training_metrics = {\n",
    "        \"train_action_dist\": [red_pct, yellow_pct],\n",
    "        \"train_wait_red\": red_waits,\n",
    "        \"train_wait_yellow\": yellow_waits,\n",
    "        \"avg_reward_per_episode\": avg_reward,\n",
    "        \"pct_within_threshold_red\": pct_red_within,\n",
    "        \"pct_within_threshold_yellow\": pct_yellow_within,\n",
    "        \"queue_red_avg\": queue_stats[\"red\"][\"avg\"],\n",
    "        \"queue_red_max\": queue_stats[\"red\"][\"max\"],\n",
    "        \"queue_yellow_avg\": queue_stats[\"yellow\"][\"avg\"],\n",
    "        \"queue_yellow_max\": queue_stats[\"yellow\"][\"max\"]\n",
    "    }\n",
    "\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(training_metrics, f, indent=4)\n",
    "\n",
    "    print(\"\\n[INFO] Monitoring complete. Logs in 'monitoring.log'. Metrics saved in 'training_metrics.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f99ea771-b95b-40ff-b67d-223f045c63a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type ndarray is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 171\u001b[39m\n\u001b[32m    168\u001b[39m     track_wait_time(\u001b[33m\"\u001b[39m\u001b[33myellow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(env.last_served_wait_times[\u001b[33m\"\u001b[39m\u001b[33myellow\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# Log decision\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[43mlog_decision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# Queue lengths\u001b[39;00m\n\u001b[32m    174\u001b[39m queue_lengths[\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m].append(\u001b[38;5;28mlen\u001b[39m(env.red_queue))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mlog_decision\u001b[39m\u001b[34m(obs, action, reward, info)\u001b[39m\n\u001b[32m     76\u001b[39m obs_list = obs.tolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, np.ndarray) \u001b[38;5;28;01melse\u001b[39;00m obs\n\u001b[32m     77\u001b[39m data = {\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m: time.time(),\n\u001b[32m     79\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maction\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(action),\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33madditional_info\u001b[39m\u001b[33m\"\u001b[39m: info\n\u001b[32m     88\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mDecision: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\rl_env\\Lib\\json\\__init__.py:231\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# cached encoder\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    228\u001b[39m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    230\u001b[39m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONEncoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\rl_env\\Lib\\json\\encoder.py:200\u001b[39m, in \u001b[36mJSONEncoder.encode\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    202\u001b[39m     chunks = \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\rl_env\\Lib\\json\\encoder.py:258\u001b[39m, in \u001b[36mJSONEncoder.iterencode\u001b[39m\u001b[34m(self, o, _one_shot)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    254\u001b[39m     _iterencode = _make_iterencode(\n\u001b[32m    255\u001b[39m         markers, \u001b[38;5;28mself\u001b[39m.default, _encoder, \u001b[38;5;28mself\u001b[39m.indent, floatstr,\n\u001b[32m    256\u001b[39m         \u001b[38;5;28mself\u001b[39m.key_separator, \u001b[38;5;28mself\u001b[39m.item_separator, \u001b[38;5;28mself\u001b[39m.sort_keys,\n\u001b[32m    257\u001b[39m         \u001b[38;5;28mself\u001b[39m.skipkeys, _one_shot)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\rl_env\\Lib\\json\\encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type ndarray is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from collections import deque, Counter\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# -------------------------------\n",
    "# Setup logging\n",
    "# -------------------------------\n",
    "logging.basicConfig(\n",
    "    filename=\"monitoring.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Global buffers for drift detection\n",
    "# -------------------------------\n",
    "recent_rewards = deque(maxlen=200)\n",
    "recent_actions = deque(maxlen=200)\n",
    "recent_wait_red = deque(maxlen=200)\n",
    "recent_wait_yellow = deque(maxlen=200)\n",
    "\n",
    "# -------------------------------\n",
    "# Monitoring functions\n",
    "# -------------------------------\n",
    "def track_reward(reward):\n",
    "    recent_rewards.append(reward)\n",
    "    logging.info(f\"Reward logged: {reward}\")\n",
    "\n",
    "def track_action(action):\n",
    "    recent_actions.append(action)\n",
    "    count = Counter(recent_actions)\n",
    "    total = sum(count.values())\n",
    "    dist = {\n",
    "        \"red\": count.get(0, 0) / total if total else 0,\n",
    "        \"yellow\": count.get(1, 0) / total if total else 0\n",
    "    }\n",
    "    logging.info(f\"Action distribution: {dist}\")\n",
    "\n",
    "    # Drift detection\n",
    "    action_vector = np.array([dist[\"red\"], dist[\"yellow\"]])\n",
    "    if \"train_action_dist\" in globals():\n",
    "        drift = np.linalg.norm(action_vector - train_action_dist)\n",
    "        if drift > 0.25:\n",
    "            logging.warning(\"MODEL DRIFT DETECTED: Action distribution deviates from training!\")\n",
    "\n",
    "def track_wait_time(cat, wait_time):\n",
    "    if cat == \"red\":\n",
    "        recent_wait_red.append(wait_time)\n",
    "        if len(recent_wait_red) > 30:\n",
    "            _check_wait_time_drift(\"red\")\n",
    "    else:\n",
    "        recent_wait_yellow.append(wait_time)\n",
    "        if len(recent_wait_yellow) > 30:\n",
    "            _check_wait_time_drift(\"yellow\")\n",
    "\n",
    "def _check_wait_time_drift(cat):\n",
    "    if cat == \"red\":\n",
    "        stat, p = ks_2samp(train_wait_red, list(recent_wait_red))\n",
    "    else:\n",
    "        stat, p = ks_2samp(train_wait_yellow, list(recent_wait_yellow))\n",
    "\n",
    "    if p < 0.05:\n",
    "        logging.warning(f\"DATA DRIFT DETECTED in {cat.upper()} wait times (p={p:.4f})\")\n",
    "\n",
    "def log_decision(obs, action, reward, info={}):\n",
    "    # Convert obs to list if it's a numpy array\n",
    "    obs_list = obs.tolist() if isinstance(obs, np.ndarray) else obs\n",
    "    data = {\n",
    "        \"timestamp\": time.time(),\n",
    "        \"action\": int(action),\n",
    "        \"reward\": float(reward),\n",
    "        \"free_doctors\": int(obs_list[0]),\n",
    "        \"longest_wait_red\": float(obs_list[1]),\n",
    "        \"longest_wait_yellow\": float(obs_list[2]),\n",
    "        \"red_queue_len\": int(obs_list[3]),\n",
    "        \"yellow_queue_len\": int(obs_list[4]),\n",
    "        \"doctor_busy_times\": [float(obs_list[5]), float(obs_list[6]), float(obs_list[7])],\n",
    "        \"additional_info\": info\n",
    "    }\n",
    "    logging.info(\"Decision: \" + json.dumps(data))\n",
    "\n",
    "# -------------------------------\n",
    "# Main loop: run actual environment\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths\n",
    "    root_dir = \"C:/Users/Prudence Letaru/Desktop/RL_Project_New\"\n",
    "    sys.path.append(root_dir)\n",
    "    sys.path.append(os.path.join(root_dir, \"env\"))\n",
    "\n",
    "    # Import environment\n",
    "    from hospital_env import HospitalEnv\n",
    "\n",
    "    # Create evaluation environment\n",
    "    def make_env():\n",
    "        env = HospitalEnv()\n",
    "        env = Monitor(env)\n",
    "        return env\n",
    "\n",
    "    eval_env = DummyVecEnv([make_env])\n",
    "\n",
    "    # Load trained model\n",
    "    model = DQN.load(os.path.join(root_dir, \"models/dqn_hospital_sb3\"), env=eval_env)\n",
    "\n",
    "    # Load previous training metrics if available\n",
    "    metrics_path = os.path.join(root_dir, \"training_metrics.json\")\n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path, \"r\") as f:\n",
    "            training_metrics = json.load(f)\n",
    "        train_action_dist = np.array(training_metrics[\"train_action_dist\"])\n",
    "        train_wait_red = np.array(training_metrics[\"train_wait_red\"])\n",
    "        train_wait_yellow = np.array(training_metrics[\"train_wait_yellow\"])\n",
    "    else:\n",
    "        train_action_dist = np.array([0.5, 0.5])\n",
    "        train_wait_red = np.random.normal(10, 3, 500)\n",
    "        train_wait_yellow = np.random.normal(20, 5, 500)\n",
    "\n",
    "    # Evaluation parameters\n",
    "    n_episodes = 40\n",
    "    threshold_times = {\"red\": 15, \"yellow\": 30}\n",
    "\n",
    "    # Metrics storage\n",
    "    rewards_per_episode = []\n",
    "    red_waits, yellow_waits = [], []\n",
    "    queue_lengths = {\"red\": [], \"yellow\": []}\n",
    "    total_actions_red, total_actions_yellow = 0, 0\n",
    "\n",
    "    # -------------------------------\n",
    "    # Run episodes\n",
    "    # -------------------------------\n",
    "    for ep in range(n_episodes):\n",
    "        obs = eval_env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "\n",
    "            if action[0] == 0:\n",
    "                total_actions_red += 1\n",
    "            else:\n",
    "                total_actions_yellow += 1\n",
    "\n",
    "            obs, reward, done, info = eval_env.step(action)\n",
    "            episode_reward += float(reward[0])\n",
    "\n",
    "            env = eval_env.envs[0].unwrapped\n",
    "\n",
    "            # Track metrics\n",
    "            track_reward(float(reward[0]))\n",
    "            track_action(action[0])\n",
    "\n",
    "            # Track wait times only if someone was served\n",
    "            if env.last_served_wait_times[\"red\"] > 0:\n",
    "                red_waits.append(float(env.last_served_wait_times[\"red\"]))\n",
    "                track_wait_time(\"red\", float(env.last_served_wait_times[\"red\"]))\n",
    "            if env.last_served_wait_times[\"yellow\"] > 0:\n",
    "                yellow_waits.append(float(env.last_served_wait_times[\"yellow\"]))\n",
    "                track_wait_time(\"yellow\", float(env.last_served_wait_times[\"yellow\"]))\n",
    "\n",
    "            # Log decision\n",
    "            log_decision(obs[0], action[0], float(reward[0]), info)\n",
    "\n",
    "            # Queue lengths\n",
    "            queue_lengths[\"red\"].append(len(env.red_queue))\n",
    "            queue_lengths[\"yellow\"].append(len(env.yellow_queue))\n",
    "\n",
    "        rewards_per_episode.append(episode_reward)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Compute summary metrics\n",
    "    # -------------------------------\n",
    "    avg_reward = float(np.mean(rewards_per_episode))\n",
    "    avg_wait_red = float(np.mean(red_waits)) if red_waits else 0\n",
    "    avg_wait_yellow = float(np.mean(yellow_waits)) if yellow_waits else 0\n",
    "    pct_red_within = float(100 * sum(w <= threshold_times[\"red\"] for w in red_waits) / len(red_waits)) if red_waits else 0\n",
    "    pct_yellow_within = float(100 * sum(w <= threshold_times[\"yellow\"] for w in yellow_waits) / len(yellow_waits)) if yellow_waits else 0\n",
    "    queue_stats = {cat: {\"avg\": float(np.mean(qs)), \"max\": int(np.max(qs))} for cat, qs in queue_lengths.items()}\n",
    "\n",
    "    # Action distribution\n",
    "    total_actions = total_actions_red + total_actions_yellow\n",
    "    red_pct = float(total_actions_red / total_actions) if total_actions > 0 else 0\n",
    "    yellow_pct = float(total_actions_yellow / total_actions) if total_actions > 0 else 0\n",
    "\n",
    "    # -------------------------------\n",
    "    # Print summary\n",
    "    # -------------------------------\n",
    "    print(\"\\n=== ACTION DISTRIBUTION ===\")\n",
    "    print(f\"Red actions: {total_actions_red} ({red_pct:.3f})\")\n",
    "    print(f\"Yellow actions: {total_actions_yellow} ({yellow_pct:.3f})\")\n",
    "    print(\"\\n=== PERFORMANCE METRICS ===\")\n",
    "    print(f\"Average reward per episode: {avg_reward:.2f}\")\n",
    "    print(f\"Average wait times (Red, Yellow): {avg_wait_red:.2f}, {avg_wait_yellow:.2f}\")\n",
    "    print(f\"Percentage served within thresholds (Red, Yellow): {pct_red_within:.2f}%, {pct_yellow_within:.2f}%\")\n",
    "    for cat, stats in queue_stats.items():\n",
    "        print(f\"{cat.capitalize()}: avg={stats['avg']:.2f}, max={stats['max']}\")\n",
    "\n",
    "    # Save metrics\n",
    "    training_metrics = {\n",
    "        \"train_action_dist\": [red_pct, yellow_pct],\n",
    "        \"train_wait_red\": red_waits,\n",
    "        \"train_wait_yellow\": yellow_waits,\n",
    "        \"avg_reward_per_episode\": avg_reward,\n",
    "        \"pct_within_threshold_red\": pct_red_within,\n",
    "        \"pct_within_threshold_yellow\": pct_yellow_within,\n",
    "        \"queue_red_avg\": queue_stats[\"red\"][\"avg\"],\n",
    "        \"queue_red_max\": queue_stats[\"red\"][\"max\"],\n",
    "        \"queue_yellow_avg\": queue_stats[\"yellow\"][\"avg\"],\n",
    "        \"queue_yellow_max\": queue_stats[\"yellow\"][\"max\"]\n",
    "    }\n",
    "\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(training_metrics, f, indent=4)\n",
    "\n",
    "    print(\"\\n[INFO] Monitoring complete. Logs in 'monitoring.log'. Metrics saved in 'training_metrics.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aaa91b9-4e47-479f-a3cf-7277772e5474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ACTION DISTRIBUTION ===\n",
      "Red actions: 564 (0.470)\n",
      "Yellow actions: 636 (0.530)\n",
      "\n",
      "=== PERFORMANCE METRICS ===\n",
      "Average reward per episode: 1895.70\n",
      "Average wait times (Red, Yellow): 3.90, 5.86\n",
      "Percentage served within thresholds (Red, Yellow): 100.00%, 100.00%\n",
      "Red: avg=1.85, max=9\n",
      "Yellow: avg=6.54, max=25\n",
      "\n",
      "[INFO] Monitoring complete. Logs in 'monitoring.log'. Metrics saved in 'training_metrics.json'.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from collections import deque, Counter\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# -------------------------------\n",
    "# Setup logging\n",
    "# -------------------------------\n",
    "logging.basicConfig(\n",
    "    filename=\"monitoring.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Global buffers for drift detection\n",
    "# -------------------------------\n",
    "recent_rewards = deque(maxlen=200)\n",
    "recent_actions = deque(maxlen=200)\n",
    "recent_wait_red = deque(maxlen=200)\n",
    "recent_wait_yellow = deque(maxlen=200)\n",
    "\n",
    "# -------------------------------\n",
    "# Monitoring functions\n",
    "# -------------------------------\n",
    "def track_reward(reward):\n",
    "    recent_rewards.append(reward)\n",
    "    logging.info(f\"Reward logged: {reward}\")\n",
    "\n",
    "def track_action(action):\n",
    "    recent_actions.append(action)\n",
    "    count = Counter(recent_actions)\n",
    "    total = sum(count.values())\n",
    "    dist = {\n",
    "        \"red\": count.get(0, 0) / total if total else 0,\n",
    "        \"yellow\": count.get(1, 0) / total if total else 0\n",
    "    }\n",
    "    logging.info(f\"Action distribution: {dist}\")\n",
    "\n",
    "    # Model drift detection\n",
    "    action_vector = np.array([dist[\"red\"], dist[\"yellow\"]])\n",
    "    if \"train_action_dist\" in globals():\n",
    "        drift = np.linalg.norm(action_vector - train_action_dist)\n",
    "        if drift > 0.25:\n",
    "            logging.warning(\"MODEL DRIFT DETECTED: Action distribution deviates from training!\")\n",
    "\n",
    "def track_wait_time(cat, wait_time):\n",
    "    if cat == \"red\":\n",
    "        recent_wait_red.append(wait_time)\n",
    "        if len(recent_wait_red) > 30:\n",
    "            _check_wait_time_drift(\"red\")\n",
    "    else:\n",
    "        recent_wait_yellow.append(wait_time)\n",
    "        if len(recent_wait_yellow) > 30:\n",
    "            _check_wait_time_drift(\"yellow\")\n",
    "\n",
    "def _check_wait_time_drift(cat):\n",
    "    if cat == \"red\":\n",
    "        stat, p = ks_2samp(train_wait_red, list(recent_wait_red))\n",
    "    else:\n",
    "        stat, p = ks_2samp(train_wait_yellow, list(recent_wait_yellow))\n",
    "\n",
    "    if p < 0.05:\n",
    "        logging.warning(f\"DATA DRIFT DETECTED in {cat.upper()} wait times (p={p:.4f})\")\n",
    "\n",
    "def log_decision(obs, action, reward, info={}):\n",
    "    # Recursive conversion of NumPy arrays to lists\n",
    "    def convert(obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: convert(v) for k, v in obj.items()}\n",
    "        if isinstance(obj, list):\n",
    "            return [convert(x) for x in obj]\n",
    "        return obj\n",
    "\n",
    "    obs = convert(obs)\n",
    "    info = convert(info)\n",
    "\n",
    "    data = {\n",
    "        \"timestamp\": time.time(),\n",
    "        \"action\": int(action),\n",
    "        \"reward\": float(reward),\n",
    "        \"free_doctors\": int(obs[0]),\n",
    "        \"longest_wait_red\": float(obs[1]),\n",
    "        \"longest_wait_yellow\": float(obs[2]),\n",
    "        \"red_queue_len\": int(obs[3]),\n",
    "        \"yellow_queue_len\": int(obs[4]),\n",
    "        \"doctor_busy_times\": [float(obs[5]), float(obs[6]), float(obs[7])],\n",
    "        \"additional_info\": info\n",
    "    }\n",
    "    logging.info(\"Decision: \" + json.dumps(data))\n",
    "\n",
    "# -------------------------------\n",
    "# Main loop: run actual environment\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = \"C:/Users/Prudence Letaru/Desktop/RL_Project_New\"\n",
    "    sys.path.append(root_dir)\n",
    "    sys.path.append(os.path.join(root_dir, \"env\"))\n",
    "\n",
    "    from hospital_env import HospitalEnv\n",
    "\n",
    "    # Create evaluation environment\n",
    "    def make_env():\n",
    "        env = HospitalEnv()\n",
    "        env = Monitor(env)\n",
    "        return env\n",
    "\n",
    "    eval_env = DummyVecEnv([make_env])\n",
    "\n",
    "    # Load trained model\n",
    "    model = DQN.load(os.path.join(root_dir, \"models/dqn_hospital_sb3\"), env=eval_env)\n",
    "\n",
    "    # Load previous training metrics if available\n",
    "    metrics_file = os.path.join(root_dir, \"training_metrics.json\")\n",
    "    if os.path.exists(metrics_file):\n",
    "        with open(metrics_file, \"r\") as f:\n",
    "            training_metrics = json.load(f)\n",
    "        train_action_dist = np.array(training_metrics[\"train_action_dist\"])\n",
    "        train_wait_red = np.array(training_metrics[\"train_wait_red\"])\n",
    "        train_wait_yellow = np.array(training_metrics[\"train_wait_yellow\"])\n",
    "    else:\n",
    "        # fallback: uniform distribution\n",
    "        train_action_dist = np.array([0.5, 0.5])\n",
    "        train_wait_red = np.random.normal(10, 3, 500)\n",
    "        train_wait_yellow = np.random.normal(20, 5, 500)\n",
    "\n",
    "    # Evaluation parameters\n",
    "    n_episodes = 40\n",
    "    threshold_times = {\"red\": 15, \"yellow\": 30}\n",
    "\n",
    "    # Metrics storage\n",
    "    rewards_per_episode = []\n",
    "    red_waits, yellow_waits = [], []\n",
    "    queue_lengths = {\"red\": [], \"yellow\": []}\n",
    "    total_actions_red, total_actions_yellow = 0, 0\n",
    "\n",
    "    # -------------------------------\n",
    "    # Run episodes\n",
    "    # -------------------------------\n",
    "    for ep in range(n_episodes):\n",
    "        obs = eval_env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "\n",
    "            if action[0] == 0:\n",
    "                total_actions_red += 1\n",
    "            else:\n",
    "                total_actions_yellow += 1\n",
    "\n",
    "            obs, reward, done, info = eval_env.step(action)\n",
    "            episode_reward += float(reward[0])\n",
    "\n",
    "            env = eval_env.envs[0].unwrapped\n",
    "\n",
    "            # Track metrics\n",
    "            track_reward(float(reward[0]))\n",
    "            track_action(action[0])\n",
    "\n",
    "            # Track wait times only if someone was served\n",
    "            if env.last_served_wait_times[\"red\"] > 0:\n",
    "                red_waits.append(float(env.last_served_wait_times[\"red\"]))\n",
    "                track_wait_time(\"red\", float(env.last_served_wait_times[\"red\"]))\n",
    "            if env.last_served_wait_times[\"yellow\"] > 0:\n",
    "                yellow_waits.append(float(env.last_served_wait_times[\"yellow\"]))\n",
    "                track_wait_time(\"yellow\", float(env.last_served_wait_times[\"yellow\"]))\n",
    "\n",
    "            # Log decision (obs converted inside function)\n",
    "            log_decision(obs[0], action[0], float(reward[0]), info)\n",
    "\n",
    "            # Queue lengths\n",
    "            queue_lengths[\"red\"].append(len(env.red_queue))\n",
    "            queue_lengths[\"yellow\"].append(len(env.yellow_queue))\n",
    "\n",
    "        rewards_per_episode.append(episode_reward)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Compute summary metrics\n",
    "    # -------------------------------\n",
    "    avg_reward = float(np.mean(rewards_per_episode))\n",
    "    avg_wait_red = float(np.mean(red_waits)) if red_waits else 0\n",
    "    avg_wait_yellow = float(np.mean(yellow_waits)) if yellow_waits else 0\n",
    "    pct_red_within = float(100 * sum(w <= threshold_times[\"red\"] for w in red_waits) / len(red_waits)) if red_waits else 0\n",
    "    pct_yellow_within = float(100 * sum(w <= threshold_times[\"yellow\"] for w in yellow_waits) / len(yellow_waits)) if yellow_waits else 0\n",
    "    queue_stats = {cat: {\"avg\": float(np.mean(qs)), \"max\": int(np.max(qs))} for cat, qs in queue_lengths.items()}\n",
    "\n",
    "    # Action distribution\n",
    "    total_actions = total_actions_red + total_actions_yellow\n",
    "    red_pct = float(total_actions_red / total_actions) if total_actions > 0 else 0\n",
    "    yellow_pct = float(total_actions_yellow / total_actions) if total_actions > 0 else 0\n",
    "\n",
    "    # -------------------------------\n",
    "    # Print summary\n",
    "    # -------------------------------\n",
    "    print(\"\\n=== ACTION DISTRIBUTION ===\")\n",
    "    print(f\"Red actions: {total_actions_red} ({red_pct:.3f})\")\n",
    "    print(f\"Yellow actions: {total_actions_yellow} ({yellow_pct:.3f})\")\n",
    "    print(\"\\n=== PERFORMANCE METRICS ===\")\n",
    "    print(f\"Average reward per episode: {avg_reward:.2f}\")\n",
    "    print(f\"Average wait times (Red, Yellow): {avg_wait_red:.2f}, {avg_wait_yellow:.2f}\")\n",
    "    print(f\"Percentage served within thresholds (Red, Yellow): {pct_red_within:.2f}%, {pct_yellow_within:.2f}%\")\n",
    "    for cat, stats in queue_stats.items():\n",
    "        print(f\"{cat.capitalize()}: avg={stats['avg']:.2f}, max={stats['max']}\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # Save metrics\n",
    "    # -------------------------------\n",
    "    training_metrics = {\n",
    "        \"train_action_dist\": [red_pct, yellow_pct],\n",
    "        \"train_wait_red\": red_waits,\n",
    "        \"train_wait_yellow\": yellow_waits,\n",
    "        \"avg_reward_per_episode\": avg_reward,\n",
    "        \"pct_within_threshold_red\": pct_red_within,\n",
    "        \"pct_within_threshold_yellow\": pct_yellow_within,\n",
    "        \"queue_red_avg\": queue_stats[\"red\"][\"avg\"],\n",
    "        \"queue_red_max\": queue_stats[\"red\"][\"max\"],\n",
    "        \"queue_yellow_avg\": queue_stats[\"yellow\"][\"avg\"],\n",
    "        \"queue_yellow_max\": queue_stats[\"yellow\"][\"max\"]\n",
    "    }\n",
    "\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(training_metrics, f, indent=4)\n",
    "\n",
    "    print(\"\\n[INFO] Monitoring complete. Logs in 'monitoring.log'. Metrics saved in 'training_metrics.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1463cf3-e0ff-4ac9-a99f-fff87896c443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Continuous monitoring started. Press Ctrl+C to stop.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 112\u001b[39m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(recent_rewards) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m processed_lines % \u001b[32m20\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m    108\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[SUMMARY] Mean reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.mean(recent_rewards)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    109\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAction distribution: RED \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecent_actions.count(\u001b[32m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, YELLOW \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecent_actions.count(\u001b[32m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    110\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAvg waits: RED \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.mean(recent_wait_red)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, YELLOW \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.mean(recent_wait_yellow)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# check every 2 seconds\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque, Counter\n",
    "from scipy.stats import ks_2samp\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# Setup logging\n",
    "# -------------------------------\n",
    "logging.basicConfig(\n",
    "    filename=\"monitoring.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Load training metrics\n",
    "# -------------------------------\n",
    "METRICS_FILE = \"training_metrics.json\"\n",
    "if os.path.exists(METRICS_FILE):\n",
    "    with open(METRICS_FILE, \"r\") as f:\n",
    "        training_metrics = json.load(f)\n",
    "    train_action_dist = np.array(training_metrics[\"train_action_dist\"])\n",
    "    train_wait_red = np.array(training_metrics[\"train_wait_red\"])\n",
    "    train_wait_yellow = np.array(training_metrics[\"train_wait_yellow\"])\n",
    "else:\n",
    "    train_action_dist = np.array([0.5, 0.5])\n",
    "    train_wait_red = np.random.normal(10, 3, 500)\n",
    "    train_wait_yellow = np.random.normal(20, 5, 500)\n",
    "\n",
    "# -------------------------------\n",
    "# Monitoring buffers\n",
    "# -------------------------------\n",
    "recent_rewards = deque(maxlen=200)\n",
    "recent_actions = deque(maxlen=200)\n",
    "recent_wait_red = deque(maxlen=200)\n",
    "recent_wait_yellow = deque(maxlen=200)\n",
    "\n",
    "# -------------------------------\n",
    "# Monitoring functions\n",
    "# -------------------------------\n",
    "def track_reward(r):\n",
    "    recent_rewards.append(r)\n",
    "\n",
    "def track_action(a):\n",
    "    recent_actions.append(a)\n",
    "    count = Counter(recent_actions)\n",
    "    total = sum(count.values())\n",
    "    dist = {\"red\": count.get(0, 0)/total if total else 0,\n",
    "            \"yellow\": count.get(1, 0)/total if total else 0}\n",
    "    # Model drift check\n",
    "    drift = np.linalg.norm(np.array([dist[\"red\"], dist[\"yellow\"]]) - train_action_dist)\n",
    "    if drift > 0.25:\n",
    "        logging.warning(\"MODEL DRIFT DETECTED\")\n",
    "    return dist\n",
    "\n",
    "def track_wait(cat, wt):\n",
    "    if cat == \"red\":\n",
    "        recent_wait_red.append(wt)\n",
    "        if len(recent_wait_red) > 30:\n",
    "            check_wait_drift(\"red\")\n",
    "    else:\n",
    "        recent_wait_yellow.append(wt)\n",
    "        if len(recent_wait_yellow) > 30:\n",
    "            check_wait_drift(\"yellow\")\n",
    "\n",
    "def check_wait_drift(cat):\n",
    "    if cat == \"red\":\n",
    "        stat, p = ks_2samp(train_wait_red, list(recent_wait_red))\n",
    "    else:\n",
    "        stat, p = ks_2samp(train_wait_yellow, list(recent_wait_yellow))\n",
    "    if p < 0.05:\n",
    "        logging.warning(f\"DATA DRIFT DETECTED in {cat.upper()} wait times (p={p:.4f})\")\n",
    "\n",
    "# -------------------------------\n",
    "# Monitor API logs in real time\n",
    "# -------------------------------\n",
    "LOG_FILE = \"api_logs.json\"\n",
    "processed_lines = 0\n",
    "\n",
    "print(\"[INFO] Continuous monitoring started. Press Ctrl+C to stop.\")\n",
    "\n",
    "while True:\n",
    "    if os.path.exists(LOG_FILE):\n",
    "        with open(LOG_FILE, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        new_lines = lines[processed_lines:]\n",
    "        for line in new_lines:\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "                action = 0 if entry[\"action\"] == \"RED\" else 1\n",
    "                reward = entry[\"reward\"]\n",
    "                wait_time = entry[\"wait_time\"]\n",
    "                cat = \"red\" if action == 0 else \"yellow\"\n",
    "\n",
    "                track_action(action)\n",
    "                track_wait(cat, wait_time)\n",
    "                track_reward(reward)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing log line: {e}\")\n",
    "\n",
    "        processed_lines += len(new_lines)\n",
    "\n",
    "        # Optional: print periodic summary\n",
    "        if len(recent_rewards) > 0 and processed_lines % 20 == 0:\n",
    "            print(f\"[SUMMARY] Mean reward: {np.mean(recent_rewards):.2f}, \"\n",
    "                  f\"Action distribution: RED {recent_actions.count(0)}, YELLOW {recent_actions.count(1)}, \"\n",
    "                  f\"Avg waits: RED {np.mean(recent_wait_red):.2f}, YELLOW {np.mean(recent_wait_yellow):.2f}\")\n",
    "\n",
    "    time.sleep(2)  # check every 2 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0291743f-a226-410c-ac06-d94d4aeff18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from stable_baselines3 import DQN\n",
    "from hospital_env import HospitalEnv\n",
    "\n",
    "# -------------------------------\n",
    "# FastAPI app\n",
    "# -------------------------------\n",
    "app = FastAPI(title=\"Hospital RL Agent API\")\n",
    "\n",
    "# -------------------------------\n",
    "# Pydantic models for input\n",
    "# -------------------------------\n",
    "class State(BaseModel):\n",
    "    free_doctors: int\n",
    "    longest_wait_red: float\n",
    "    longest_wait_yellow: float\n",
    "    red_queue_length: int\n",
    "    yellow_queue_length: int\n",
    "    doctor1_busy_time: float\n",
    "    doctor2_busy_time: float\n",
    "    doctor3_busy_time: float\n",
    "\n",
    "class RequestBody(BaseModel):\n",
    "    state: State\n",
    "\n",
    "# -------------------------------\n",
    "# Load trained RL model\n",
    "# -------------------------------\n",
    "MODEL_PATH = \"models/dqn_hospital_sb3\"\n",
    "model = DQN.load(MODEL_PATH)\n",
    "\n",
    "# -------------------------------\n",
    "# Logging location for monitoring\n",
    "# -------------------------------\n",
    "LOG_FILE = \"api_logs.json\"\n",
    "\n",
    "# -------------------------------\n",
    "# Convert API state to environment observation\n",
    "# -------------------------------\n",
    "def state_to_obs(state: State):\n",
    "    return np.array([\n",
    "        state.free_doctors,\n",
    "        state.longest_wait_red,\n",
    "        state.longest_wait_yellow,\n",
    "        state.red_queue_length,\n",
    "        state.yellow_queue_length,\n",
    "        state.doctor1_busy_time,\n",
    "        state.doctor2_busy_time,\n",
    "        state.doctor3_busy_time\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# API endpoint\n",
    "# -------------------------------\n",
    "@app.post(\"/predict\")\n",
    "def predict(request: RequestBody):\n",
    "    obs = state_to_obs(request.state)\n",
    "\n",
    "    # Predict action\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    action = int(action)\n",
    "\n",
    "    # Compute reward and wait time using environment logic\n",
    "    # Here we simulate an environment step\n",
    "    env = HospitalEnv()\n",
    "    # Set environment state manually\n",
    "    env.doctor_timers = np.array([request.state.doctor1_busy_time,\n",
    "                                  request.state.doctor2_busy_time,\n",
    "                                  request.state.doctor3_busy_time], dtype=np.float32)\n",
    "    env.red_queue = [request.state.longest_wait_red]*request.state.red_queue_length\n",
    "    env.yellow_queue = [request.state.longest_wait_yellow]*request.state.yellow_queue_length\n",
    "\n",
    "    _, reward, _, _, _ = env.step(action)\n",
    "    wait_time = env.last_served_wait_times[\"red\"] if action == 0 else env.last_served_wait_times[\"yellow\"]\n",
    "\n",
    "    # Log to file for monitoring\n",
    "    log_entry = {\n",
    "        \"timestamp\": time.time(),\n",
    "        \"state\": request.state.dict(),\n",
    "        \"action\": \"RED\" if action == 0 else \"YELLOW\",\n",
    "        \"reward\": reward,\n",
    "        \"wait_time\": wait_time\n",
    "    }\n",
    "    with open(LOG_FILE, \"a\") as f:\n",
    "        f.write(json.dumps(log_entry) + \"\\n\")\n",
    "\n",
    "    return {\n",
    "        \"action\": \"RED\" if action == 0 else \"YELLOW\",\n",
    "        \"reward\": reward,\n",
    "        \"wait_time\": wait_time\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd573a0f-48c2-4eb5-84ff-15d6b611116e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rl_env)",
   "language": "python",
   "name": "rl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
